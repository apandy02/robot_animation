{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad.nn\n",
    "import numpy as np\n",
    "from tinygrad import nn , Tensor\n",
    "\n",
    "\n",
    "\n",
    "def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):\n",
    "    \"\"\"CleanRL's default layer initialization\"\"\"\n",
    "    layer.weight = tiny_orthogonal_(layer.weight, std)\n",
    "    layer.bias = tiny_constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "from tinygrad import nn \n",
    "def tiny_orthogonal_(tensor: Tensor, gain=1, generator=None):\n",
    "    \"\"\"\n",
    "    NOTE: Since initialization occurs only once, we are being lazy and using numpy linear algebra to perform certain operations.\n",
    "    \"\"\"\n",
    "    if tensor.ndim < 2:\n",
    "        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n",
    "\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor # no-op for empty tensors\n",
    "\n",
    "    rows, cols = tensor.shape[0], tensor.numel() // tensor.shape[0]\n",
    "    flattened = Tensor.randn(rows, cols) # figure out if it has the same device configs as the input tensor\n",
    "\n",
    "    if rows < cols:\n",
    "        flattened = flattened.transpose()\n",
    "\n",
    "    # for now, we use numpy to compute the qr factorization\n",
    "    q, r = np.linalg.qr(flattened.numpy())\n",
    "\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q.transpose()\n",
    "\n",
    "    return Tensor(q).mul(gain)\n",
    "\n",
    "def tiny_constant_(tensor: Tensor, val: float):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return Tensor.ones(tensor.shape) * val\n",
    "    \n",
    "\n",
    "from tinygrad import nn \n",
    "\n",
    "\n",
    "class TinyPolicy:\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.is_continuous = hasattr(policy, 'is_continuous') and policy.is_continuous\n",
    "    \n",
    "    def __call__(self, x, action=None):\n",
    "        return self.get_action_and_value(x, action)\n",
    "\n",
    "    def get_value(self, x, state=None):\n",
    "        _, value = self.policy(x)\n",
    "        return value\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits, value = self.policy(x)\n",
    "        action, logprob, entropy = sample_logits(logits, action, self.is_continuous)\n",
    "        return action, logprob, entropy, value\n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "        self.l3 = layer_init(tinygrad.nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        x = self.l2(x).tanh()\n",
    "        return self.l3(x)\n",
    "\n",
    "class ActorEncoder:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        return self.l2(x).tanh()\n",
    "\n",
    "class TinyCleanRLPolicy(TinyPolicy):\n",
    "    def __init__(self, envs, hidden_size=64):\n",
    "        super().__init__(policy=None)  # Just to get the right init\n",
    "        self.is_continuous = True\n",
    "\n",
    "        # self.obs_size = np.array(envs.single_observation_space.shape).prod()\n",
    "        # action_size = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        ## figuring out how to normalize observations will be an important step, but leaving it out for now\n",
    "        action_size = 1\n",
    "        self.obs_size = 1\n",
    "        self.critic = Critic(self.obs_size, hidden_size)\n",
    "        self.actor_encoder = ActorEncoder(self.obs_size, hidden_size)\n",
    "        self.actor_decoder_mean = layer_init(tinygrad.nn.Linear(hidden_size, action_size), std=0.01)\n",
    "        self.actor_decoder_logstd = Tensor.zeros(1, action_size)\n",
    "\n",
    "\n",
    "\n",
    "policy = TinyCleanRLPolicy(\"hypothetical env\", hidden_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import torch\n",
    "a = Normal(torch.tensor([1.0, 2.0]), torch.tensor([1.0, 1.0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "a.log_prob(torch.tensor([1.0, 2.0])).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of c (10, 3)\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import TinyJit, Tensor\n",
    "weight = Tensor.randn(10, 3)\n",
    "@TinyJit\n",
    "def forward(x: Tensor, indices: list[int]):\n",
    "  c = (x[indices] * weight).contiguous()\n",
    "  print(f\"shape of c {c.shape}\")\n",
    "  c.sum(0).realize()\n",
    "\n",
    "x = Tensor.randn(10)\n",
    "forward(x, [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad impor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Get MNIST data \n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "transform = transforms.ToTensor()\n",
    "mnist_train = datasets.MNIST('./', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST('./', train=False, transform=transform)\n",
    "\n",
    "X_train, Y_train = mnist_train.data.float()/255, mnist_train.targets\n",
    "X_test, Y_test = mnist_test.data.float()/255, mnist_test.targets\n",
    "X_train = X_train.unsqueeze(1).to(device) # Add channel dimension\n",
    "X_test = X_test.unsqueeze(1).to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "Y_test = Y_test.to(device)\n",
    "\n",
    "print(\"Using device: mps\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.l2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.l3 = nn.Linear(1600, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.max_pool2d(torch.relu(self.l1(x)), 2)\n",
    "        x = torch.max_pool2d(torch.relu(self.l2(x)), 2)\n",
    "        x = self.dropout(x.flatten(1))\n",
    "        return self.l3(x)\n",
    "\n",
    "model = Model().to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "\n",
    "batch_size = 128\n",
    "def step():\n",
    "    model.train()\n",
    "    indices = torch.randint(len(X_train), (batch_size,))\n",
    "    X, Y = X_train[indices], Y_train[indices]\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss = nn.functional.cross_entropy(model(X), Y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "for step_num in range(5000):\n",
    "    loss = step()\n",
    "    if step_num%100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc = (model(X_test).argmax(dim=1) == Y_test).float().mean().item()\n",
    "        print(f\"step {step_num:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tinygrad MNIST implementation\n",
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "from tinygrad.nn.datasets import mnist\n",
    "import time\n",
    "\n",
    "print(\"Using device:\", Device.DEFAULT)\n",
    "\n",
    "# Get MNIST data\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n",
    "        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n",
    "        self.l3 = nn.Linear(1600, 10)\n",
    "\n",
    "    def __call__(self, x:Tensor) -> Tensor:\n",
    "        x = self.l1(x).relu().max_pool2d((2,2))\n",
    "        x = self.l2(x).relu().max_pool2d((2,2))\n",
    "        return self.l3(x.flatten(1).dropout(0.5))\n",
    "\n",
    "model = Model()\n",
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def step():\n",
    "    Tensor.training = True  # makes dropout work\n",
    "    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "    X, Y = X_train[samples], Y_train[samples]\n",
    "    optim.zero_grad()\n",
    "    loss = model(X).sparse_categorical_crossentropy(Y).backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "# JIT compile the training step\n",
    "jit_step = TinyJit(step)\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "for step_num in range(5000):\n",
    "    loss = jit_step()\n",
    "    if step_num%100 == 0:\n",
    "        Tensor.training = False\n",
    "        acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "        print(f\"step {step_num:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
