{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad.nn\n",
    "import numpy as np\n",
    "from tinygrad import nn , Tensor\n",
    "\n",
    "\n",
    "\n",
    "def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):\n",
    "    \"\"\"CleanRL's default layer initialization\"\"\"\n",
    "    layer.weight = tiny_orthogonal_(layer.weight, std)\n",
    "    layer.bias = tiny_constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "from tinygrad import nn \n",
    "def tiny_orthogonal_(tensor: Tensor, gain=1, generator=None):\n",
    "    \"\"\"\n",
    "    NOTE: Since initialization occurs only once, we are being lazy and using numpy linear algebra to perform certain operations.\n",
    "    \"\"\"\n",
    "    if tensor.ndim < 2:\n",
    "        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n",
    "\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor # no-op for empty tensors\n",
    "\n",
    "    rows, cols = tensor.shape[0], tensor.numel() // tensor.shape[0]\n",
    "    flattened = Tensor.randn(rows, cols) # figure out if it has the same device configs as the input tensor\n",
    "\n",
    "    if rows < cols:\n",
    "        flattened = flattened.transpose()\n",
    "\n",
    "    # for now, we use numpy to compute the qr factorization\n",
    "    q, r = np.linalg.qr(flattened.numpy())\n",
    "\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q.transpose()\n",
    "\n",
    "    return Tensor(q).mul(gain)\n",
    "\n",
    "def tiny_constant_(tensor: Tensor, val: float):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return Tensor.ones(tensor.shape) * val\n",
    "    \n",
    "\n",
    "from tinygrad import nn \n",
    "\n",
    "\n",
    "class TinyPolicy:\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.is_continuous = hasattr(policy, 'is_continuous') and policy.is_continuous\n",
    "    \n",
    "    def __call__(self, x, action=None):\n",
    "        return self.get_action_and_value(x, action)\n",
    "\n",
    "    def get_value(self, x, state=None):\n",
    "        _, value = self.policy(x)\n",
    "        return value\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits, value = self.policy(x)\n",
    "        action, logprob, entropy = sample_logits(logits, action, self.is_continuous)\n",
    "        return action, logprob, entropy, value\n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "        self.l3 = layer_init(tinygrad.nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        x = self.l2(x).tanh()\n",
    "        return self.l3(x)\n",
    "\n",
    "class ActorEncoder:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        return self.l2(x).tanh()\n",
    "\n",
    "class TinyCleanRLPolicy(TinyPolicy):\n",
    "    def __init__(self, envs, hidden_size=64):\n",
    "        super().__init__(policy=None)  # Just to get the right init\n",
    "        self.is_continuous = True\n",
    "\n",
    "        # self.obs_size = np.array(envs.single_observation_space.shape).prod()\n",
    "        # action_size = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        ## figuring out how to normalize observations will be an important step, but leaving it out for now\n",
    "        action_size = 1\n",
    "        self.obs_size = 1\n",
    "        self.critic = Critic(self.obs_size, hidden_size)\n",
    "        self.actor_encoder = ActorEncoder(self.obs_size, hidden_size)\n",
    "        self.actor_decoder_mean = layer_init(tinygrad.nn.Linear(hidden_size, action_size), std=0.01)\n",
    "        self.actor_decoder_logstd = Tensor.zeros(1, action_size)\n",
    "\n",
    "\n",
    "\n",
    "policy = TinyCleanRLPolicy(\"hypothetical env\", hidden_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 64) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 64) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1, 1) float ShapeTracker(views=(View(shape=(1, 1), strides=(0, 0), offset=0, mask=None, contiguous=True),))> on METAL with grad None>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.state.get_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using clip coefficient: 0.17332202196121216\n",
      "tensor([ 1.0713,  1.2626,  0.0093, -0.1674,  0.4171,  0.0372,  0.8239, -0.4644,\n",
      "         1.1956,  0.2168])\n",
      "tensor(0.4402)\n"
     ]
    }
   ],
   "source": [
    "# Policy loss\n",
    "import torch\n",
    "# Create dummy tensors with same shape\n",
    "mb_advantages = torch.randn(10) # batch size of 10 for example\n",
    "ratio = torch.ones(10) + torch.randn(10) * 0.1 # ratios close to 1.0\n",
    "\n",
    "# Random clip coefficient between 0.1 and 0.3 (typical PPO range)\n",
    "args = type('Args', (), {'clip_coef': float(torch.rand(1) * 0.2 + 0.1)})()\n",
    "print(f\"Using clip coefficient: {args.clip_coef}\")\n",
    "\n",
    "\n",
    "pg_loss1 = -mb_advantages * ratio\n",
    "pg_loss2 = -mb_advantages * ratio.clamp(1 - args.clip_coef, 1 + args.clip_coef)\n",
    "pg_loss_elementwise = torch.max(pg_loss1, pg_loss2)\n",
    "print(f\"pg_loss1: {pg_loss1}, pg_loss2: {pg_loss2}\")\n",
    "print(f\"pg_loss_elementwise: {pg_loss_elementwise}\")\n",
    "pg_loss = pg_loss_elementwise.mean()\n",
    "print(f\"pg_loss: {pg_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
