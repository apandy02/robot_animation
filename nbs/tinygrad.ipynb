{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad.nn\n",
    "import numpy as np\n",
    "from tinygrad import nn , Tensor\n",
    "\n",
    "\n",
    "\n",
    "def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):\n",
    "    \"\"\"CleanRL's default layer initialization\"\"\"\n",
    "    layer.weight = tiny_orthogonal_(layer.weight, std)\n",
    "    layer.bias = tiny_constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "from tinygrad import nn \n",
    "def tiny_orthogonal_(tensor: Tensor, gain=1, generator=None):\n",
    "    \"\"\"\n",
    "    NOTE: Since initialization occurs only once, we are being lazy and using numpy linear algebra to perform certain operations.\n",
    "    \"\"\"\n",
    "    if tensor.ndim < 2:\n",
    "        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n",
    "\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor # no-op for empty tensors\n",
    "\n",
    "    rows, cols = tensor.shape[0], tensor.numel() // tensor.shape[0]\n",
    "    flattened = Tensor.randn(rows, cols) # figure out if it has the same device configs as the input tensor\n",
    "\n",
    "    if rows < cols:\n",
    "        flattened = flattened.transpose()\n",
    "\n",
    "    # for now, we use numpy to compute the qr factorization\n",
    "    q, r = np.linalg.qr(flattened.numpy())\n",
    "\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q.transpose()\n",
    "\n",
    "    return Tensor(q).mul(gain)\n",
    "\n",
    "def tiny_constant_(tensor: Tensor, val: float):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return Tensor.ones(tensor.shape) * val\n",
    "    \n",
    "\n",
    "from tinygrad import nn \n",
    "\n",
    "\n",
    "class TinyPolicy:\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.is_continuous = hasattr(policy, 'is_continuous') and policy.is_continuous\n",
    "    \n",
    "    def __call__(self, x, action=None):\n",
    "        return self.get_action_and_value(x, action)\n",
    "\n",
    "    def get_value(self, x, state=None):\n",
    "        _, value = self.policy(x)\n",
    "        return value\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits, value = self.policy(x)\n",
    "        action, logprob, entropy = sample_logits(logits, action, self.is_continuous)\n",
    "        return action, logprob, entropy, value\n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "        self.l3 = layer_init(tinygrad.nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        x = self.l2(x).tanh()\n",
    "        return self.l3(x)\n",
    "\n",
    "class ActorEncoder:\n",
    "    def __init__(self, obs_size, hidden_size):\n",
    "        self.l1 = layer_init(tinygrad.nn.Linear(obs_size, hidden_size))\n",
    "        self.l2 = layer_init(tinygrad.nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        x = self.l1(x).tanh()\n",
    "        return self.l2(x).tanh()\n",
    "\n",
    "class TinyCleanRLPolicy(TinyPolicy):\n",
    "    def __init__(self, envs, hidden_size=64):\n",
    "        super().__init__(policy=None)  # Just to get the right init\n",
    "        self.is_continuous = True\n",
    "\n",
    "        # self.obs_size = np.array(envs.single_observation_space.shape).prod()\n",
    "        # action_size = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        ## figuring out how to normalize observations will be an important step, but leaving it out for now\n",
    "        action_size = 1\n",
    "        self.obs_size = 1\n",
    "        self.critic = Critic(self.obs_size, hidden_size)\n",
    "        self.actor_encoder = ActorEncoder(self.obs_size, hidden_size)\n",
    "        self.actor_decoder_mean = layer_init(tinygrad.nn.Linear(hidden_size, action_size), std=0.01)\n",
    "        self.actor_decoder_logstd = Tensor.zeros(1, action_size)\n",
    "\n",
    "\n",
    "\n",
    "policy = TinyCleanRLPolicy(\"hypothetical env\", hidden_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 64) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 64) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (64, 1) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1,) float (<Ops.MUL: 49>, None)> on METAL with grad None>,\n",
       " <Tensor <UOp METAL (1, 1) float ShapeTracker(views=(View(shape=(1, 1), strides=(0, 0), offset=0, mask=None, contiguous=True),))> on METAL with grad None>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.state.get_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using clip coefficient: 0.2\n",
      "Ratios before clipping: tensor([0.7646, 1.4082, 1.5149, 0.9010, 0.5702, 0.5831, 1.8986, 1.6615, 1.0109,\n",
      "        1.0097])\n",
      "Ratios after clipping: tensor([0.8000, 1.2000, 1.2000, 0.9010, 0.8000, 0.8000, 1.2000, 1.2000, 1.0109,\n",
      "        1.0097])\n",
      "pg_loss1: tensor([ 0.5812,  2.2191, -0.3515,  0.2989,  0.3137,  0.2744,  1.3776,  1.1436,\n",
      "        -0.6129, -0.1148]), pg_loss2: tensor([ 0.6081,  1.8910, -0.2784,  0.2989,  0.4402,  0.3765,  0.8707,  0.8260,\n",
      "        -0.6129, -0.1148])\n",
      "pg_loss_elementwise: tensor([ 0.6081,  2.2191, -0.2784,  0.2989,  0.4402,  0.3765,  1.3776,  1.1436,\n",
      "        -0.6129, -0.1148])\n",
      "pg_loss: 0.545782208442688\n"
     ]
    }
   ],
   "source": [
    "# Policy loss\n",
    "import torch\n",
    "# Create dummy tensors with same shape\n",
    "mb_advantages = torch.randn(10) # batch size of 10 for example\n",
    "ratio = torch.ones(10) + torch.randn(10) * 0.5 # Increased variance to 0.5 to make ratios differ more from 1.0\n",
    "\n",
    "# Fixed clip coefficient to better demonstrate clipping effects\n",
    "args = type('Args', (), {'clip_coef': 0.2})() # Fixed clip coef for clearer demonstration\n",
    "print(f\"Using clip coefficient: {args.clip_coef}\")\n",
    "\n",
    "pg_loss1 = -mb_advantages * ratio\n",
    "pg_loss2 = -mb_advantages * ratio.clamp(1 - args.clip_coef, 1 + args.clip_coef)\n",
    "pg_loss_elementwise = torch.max(pg_loss1, pg_loss2)\n",
    "print(f\"Ratios before clipping: {ratio}\")\n",
    "print(f\"Ratios after clipping: {ratio.clamp(1 - args.clip_coef, 1 + args.clip_coef)}\")\n",
    "print(f\"pg_loss1: {pg_loss1}, pg_loss2: {pg_loss2}\")\n",
    "print(f\"pg_loss_elementwise: {pg_loss_elementwise}\")\n",
    "pg_loss = pg_loss_elementwise.mean()\n",
    "print(f\"pg_loss: {pg_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [ 1.2344875   0.4698439   1.2965546  -0.67969835 -1.4338986  -0.6644178\n",
      " -0.11526925 -1.2671893   2.7294126  -2.4666617 ]\n",
      "b: [-1.2414773  -0.2149791  -1.0404463  -1.7843857   0.95054054  0.7913215\n",
      "  2.2953053   0.7872812  -0.81729704 -0.12775072]\n",
      "a.max(b): [ 1.2344875   0.4698439   1.2965546  -0.67969835  0.95054054  0.7913215\n",
      "  2.2953053   0.7872812   2.7294126  -0.12775072]\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import Tensor\n",
    "\n",
    "a = Tensor.randn(10)\n",
    "b = Tensor.randn(10)\n",
    "\n",
    "print(f\"a: {a.numpy()}\" )\n",
    "print(f\"b: {b.numpy()}\")\n",
    "\n",
    "print(f\"a.max(b): {a.maximum(b).numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn\n",
    "\n",
    "layer = nn.Linear(16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
